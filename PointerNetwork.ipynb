{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForniLucas/PointerNetwork/blob/main/PointerNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD3OfnvL-J-m"
      },
      "source": [
        "Certainly! Let's consider an example involving natural language processing (NLP), specifically, a simple language translation task.\n",
        "\n",
        "**Task**: Translate English sentences to French.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "- **Input**: \"Hello, how are you?\"\n",
        "- **Output (Desired)**: \"Bonjour, comment ça va ?\"\n",
        "\n",
        "Now, let's break down how an encoder processes this input:\n",
        "\n",
        "1. **Input Representation**:\n",
        "   - Each word in the input sentence needs to be converted into a numerical format that can be fed into a neural network. This is typically done using techniques like word embedding (e.g., Word2Vec, GloVe), where words are mapped to high-dimensional vectors.\n",
        "\n",
        "   - For example, \"Hello,\" \"how,\" \"are,\" and \"you?\" might be represented as:\n",
        "     - \"Hello\" → [0.2, -0.5, 0.8]\n",
        "     - \"how\" → [0.6, 0.1, -0.3]\n",
        "     - \"are\" → [0.4, -0.2, 0.7]\n",
        "     - \"you?\" → [0.3, -0.4, 0.9]\n",
        "\n",
        "2. **Sequence Encoding**:\n",
        "   - The encoder, which could be an LSTM or GRU, takes in these word vectors one at a time. It processes them sequentially, maintaining a hidden state that evolves with each input.\n",
        "\n",
        "   - For instance, it starts with \"Hello\":\n",
        "     - The word vector for \"Hello\" is fed in.\n",
        "     - The LSTM updates its hidden state based on this input and its previous hidden state.\n",
        "     - The updated hidden state is then used as context for processing the next word.\n",
        "\n",
        "3. **Hidden States**:\n",
        "   - As the encoder processes each word, it maintains a hidden state that encodes information about the sequence up to that point. This hidden state represents the evolving context of the input sequence.\n",
        "\n",
        "   - For example, after processing \"Hello,\" the hidden state might be:\n",
        "     - Hidden State: [0.2, -0.1, 0.6]\n",
        "\n",
        "4. **Final Hidden State** (Optional):\n",
        "   - If we're interested in summarizing the entire input sequence, we would extract the final hidden state after processing the entire sequence.\n",
        "\n",
        "   - This final hidden state is a context-aware representation of the entire input sequence.\n",
        "\n",
        "   - For example, after processing the entire input sentence, the final hidden state might be:\n",
        "     - Final Hidden State: [0.8, -0.5, 0.4]\n",
        "\n",
        "This final hidden state can be used as a representation of the entire input sentence, which can then be used for further processing in the translation task.\n",
        "\n",
        "In summary, the encoder processes the input sequence by converting each element (in this case, words) into numerical vectors, and then sequentially updates its hidden state based on these inputs. This hidden state encodes the context of the entire sequence, which is crucial for generating accurate translations in this example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qScs8eIr-NtV"
      },
      "source": [
        "Certainly! Let's consider an example of the Traveling Salesman Problem (TSP) using a pointer network. In this problem, the input is a set of cities, and the goal is to find the shortest path that visits each city exactly once and returns to the starting city.\n",
        "\n",
        "Suppose we have the following cities, represented as 2D coordinates:\n",
        "\n",
        "1. City A: (0, 0)\n",
        "2. City B: (3, 1)\n",
        "3. City C: (1, 3)\n",
        "4. City D: (4, 4)\n",
        "\n",
        "**Input** (Vector of Cities):\n",
        "```\n",
        "[\n",
        "  (0, 0),   # City A\n",
        "  (3, 1),   # City B\n",
        "  (1, 3),   # City C\n",
        "  (4, 4)    # City D\n",
        "]\n",
        "```\n",
        "\n",
        "**Encoder Process** (using an LSTM-based encoder):\n",
        "\n",
        "1. **Prepare Input Sequences**:\n",
        "   - Each city's coordinates are converted into a feature vector. For example, you could flatten the coordinates to get a 1D vector.\n",
        "\n",
        "2. **Create Input Tensor**:\n",
        "   - The input tensor will be of shape `(seq_len, input_dim)`, where `seq_len` is the number of cities (in this case, 4) and `input_dim` is the dimensionality of each city's representation (2 in this case, for x and y coordinates).\n",
        "\n",
        "   ```\n",
        "   Input Tensor:\n",
        "   [\n",
        "    [0, 0],   # City A\n",
        "    [3, 1],   # City B\n",
        "    [1, 3],   # City C\n",
        "    [4, 4]    # City D\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "3. **Initialize Hidden State**:\n",
        "   - Initialize the hidden state of the LSTM.\n",
        "\n",
        "4. **Pass through the Encoder**:\n",
        "   - The input tensor is passed through the LSTM. At each time step (corresponding to a city), the LSTM processes the input and updates its hidden state.\n",
        "\n",
        "   ```\n",
        "   Hidden States:\n",
        "   [\n",
        "    [h1_A, h2_A],   # Hidden state after processing City A\n",
        "    [h1_B, h2_B],   # Hidden state after processing City B\n",
        "    [h1_C, h2_C],   # Hidden state after processing City C\n",
        "    [h1_D, h2_D]    # Hidden state after processing City D\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "5. **Final Hidden State**:\n",
        "   - If you want to summarize the entire input sequence, you can use the final hidden state, which captures information about all the cities.\n",
        "\n",
        "   ```\n",
        "   Final Hidden State:\n",
        "   [h1_final, h2_final]\n",
        "   ```\n",
        "\n",
        "**Output** (Context-Aware Representations):\n",
        "The output of the encoder is the set of context-aware representations for each city. These representations encode information about the cities in the context of the entire sequence.\n",
        "\n",
        "```\n",
        "[\n",
        " [h1_A, h2_A],   # Context for City A\n",
        " [h1_B, h2_B],   # Context for City B\n",
        " [h1_C, h2_C],   # Context for City C\n",
        " [h1_D, h2_D]    # Context for City D\n",
        "]\n",
        "```\n",
        "\n",
        "These context-aware representations will be used in conjunction with the attention and pointer mechanisms in the subsequent stages of the pointer network to generate the output sequence, which would be the ordered list of cities representing the optimal tour in the TSP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHcBOhlI-TOI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def tsp_opt(points):\n",
        "    \"\"\"\n",
        "    Dynamic programing solution for TSP - O(2^n*n^2)\n",
        "    https://gist.github.com/mlalevic/6222750\n",
        "\n",
        "    :param points: List of (x, y) points\n",
        "    :return: Optimal solution\n",
        "    \"\"\"\n",
        "\n",
        "    def length(x_coord, y_coord):\n",
        "        return np.linalg.norm(np.asarray(x_coord) - np.asarray(y_coord))\n",
        "\n",
        "    # Calculate all lengths\n",
        "    all_distances = [[length(x, y) for y in points] for x in points]\n",
        "    # Initial value - just distance from 0 to every other point + keep the track of edges\n",
        "    A = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1]) for idx, dist in enumerate(all_distances[0][1:])}\n",
        "    cnt = len(points)\n",
        "    for m in range(2, cnt):\n",
        "        B = {}\n",
        "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
        "            for j in S - {0}:\n",
        "                # This will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
        "                B[(S, j)] = min([(A[(S-{j}, k)][0] + all_distances[k][j], A[(S-{j}, k)][1] + [j])\n",
        "                                 for k in S if k != 0 and k != j])\n",
        "        A = B\n",
        "    res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n",
        "    return np.asarray(res[1])\n",
        "\n",
        "\n",
        "class TSPDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Random TSP dataset\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_size, seq_len, solver=tsp_opt, solve=True):\n",
        "        self.data_size = data_size\n",
        "        self.seq_len = seq_len\n",
        "        self.solve = solve\n",
        "        self.solver = solver\n",
        "        self.data = self._generate_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tensor = torch.from_numpy(self.data['Points_List'][idx]).float()\n",
        "        solution = torch.from_numpy(self.data['Solutions'][idx]).long() if self.solve else None\n",
        "\n",
        "        sample = {'Points':tensor, 'Solution':solution}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def _generate_data(self):\n",
        "        \"\"\"\n",
        "        :return: Set of points_list ans their One-Hot vector solutions\n",
        "        \"\"\"\n",
        "        points_list = []\n",
        "        solutions = []\n",
        "        data_iter = tqdm(range(self.data_size), unit='data')\n",
        "        for i, _ in enumerate(data_iter):\n",
        "            data_iter.set_description('Data points %i/%i' % (i+1, self.data_size))\n",
        "            points_list.append(np.random.random((self.seq_len, 2)))\n",
        "        solutions_iter = tqdm(points_list, unit='solve')\n",
        "        if self.solve:\n",
        "            for i, points in enumerate(solutions_iter):\n",
        "                solutions_iter.set_description('Solved %i/%i' % (i+1, len(points_list)))\n",
        "                solutions.append(self.solver(points))\n",
        "        else:\n",
        "            solutions = None\n",
        "\n",
        "        return {'Points_List':points_list, 'Solutions':solutions}\n",
        "\n",
        "    def _to1hotvec(self, points):\n",
        "        \"\"\"\n",
        "        :param points: List of integers representing the points indexes\n",
        "        :return: Matrix of One-Hot vectors\n",
        "        \"\"\"\n",
        "        vec = np.zeros((len(points), self.seq_len))\n",
        "        for i, v in enumerate(vec):\n",
        "            v[points[i]] = 1\n",
        "\n",
        "        return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAfOh7q_Bzwr"
      },
      "outputs": [],
      "source": [
        "def test_tsp_opt():\n",
        "    points = [(0, 0), (1, 3), (5, 2), (4, 1)]\n",
        "    result = tsp_opt(points)\n",
        "    print(\"Ruta optima:\", result)\n",
        "\n",
        "def test_TSPDataset():\n",
        "    data_size = 5\n",
        "    seq_len = 4\n",
        "    dataset = TSPDataset(data_size, seq_len)\n",
        "    for i in range(data_size):\n",
        "        sample = dataset[i]\n",
        "        print(f\"Muestra {i+1}:\")\n",
        "        print(\"Ciudades:\")\n",
        "        print(sample['Points'])\n",
        "        print(\"Solucion:\")\n",
        "        print(sample['Solution'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OYxyQb_COYW",
        "outputId": "7d1899e7-c603-4dff-8a1f-74af3f30e54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta optima: [0 3 2 1]\n"
          ]
        }
      ],
      "source": [
        "test_tsp_opt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CsMM5FHCLp3",
        "outputId": "3cd52c06-df43-47b6-bc03-f7ff7d9fc7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data points 5/5: 100%|██████████| 5/5 [00:00<00:00, 505.05data/s]\n",
            "Solved 5/5: 100%|██████████| 5/5 [00:00<00:00, 493.30solve/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muestra 1:\n",
            "Ciudades:\n",
            "tensor([[0.9573, 0.9158],\n",
            "        [0.0410, 0.7413],\n",
            "        [0.4997, 0.0592],\n",
            "        [0.6350, 0.2792]])\n",
            "Solucion:\n",
            "tensor([0, 1, 2, 3])\n",
            "Muestra 2:\n",
            "Ciudades:\n",
            "tensor([[0.2345, 0.0805],\n",
            "        [0.5295, 0.0614],\n",
            "        [0.7411, 0.7521],\n",
            "        [0.3599, 0.5998]])\n",
            "Solucion:\n",
            "tensor([0, 3, 2, 1])\n",
            "Muestra 3:\n",
            "Ciudades:\n",
            "tensor([[0.6946, 0.2228],\n",
            "        [0.8033, 0.8572],\n",
            "        [0.1284, 0.2217],\n",
            "        [0.9155, 0.6921]])\n",
            "Solucion:\n",
            "tensor([0, 2, 1, 3])\n",
            "Muestra 4:\n",
            "Ciudades:\n",
            "tensor([[0.9500, 0.7280],\n",
            "        [0.5487, 0.2955],\n",
            "        [0.1000, 0.7382],\n",
            "        [0.3745, 0.7575]])\n",
            "Solucion:\n",
            "tensor([0, 1, 2, 3])\n",
            "Muestra 5:\n",
            "Ciudades:\n",
            "tensor([[0.8605, 0.2325],\n",
            "        [0.3823, 0.8207],\n",
            "        [0.5898, 0.5978],\n",
            "        [0.2999, 0.5119]])\n",
            "Solucion:\n",
            "tensor([0, 2, 1, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_TSPDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwI2KHAACdXl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder class for Pointer-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 n_layers,\n",
        "                 dropout,\n",
        "                 bidir):\n",
        "        \"\"\"\n",
        "        Initiate Encoder\n",
        "\n",
        "        :param Tensor embedding_dim: Number of embbeding channels\n",
        "        :param int hidden_dim: Number of hidden units for the LSTM\n",
        "        :param int n_layers: Number of layers for LSTMs\n",
        "        :param float dropout: Float between 0-1\n",
        "        :param bool bidir: Bidirectional\n",
        "        \"\"\"\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim//2 if bidir else hidden_dim\n",
        "        self.n_layers = n_layers*2 if bidir else n_layers\n",
        "        self.bidir = bidir\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            self.hidden_dim,\n",
        "                            n_layers,\n",
        "                            dropout=dropout,\n",
        "                            bidirectional=bidir)\n",
        "\n",
        "        # Used for propagating .cuda() command\n",
        "        self.h0 = Parameter(torch.zeros(1), requires_grad=False)\n",
        "        self.c0 = Parameter(torch.zeros(1), requires_grad=False)\n",
        "\n",
        "    def forward(self, embedded_inputs,\n",
        "                hidden):\n",
        "        \"\"\"\n",
        "        Encoder - Forward-pass\n",
        "\n",
        "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
        "        :param Tensor hidden: Initiated hidden units for the LSTMs (h, c)\n",
        "        :return: LSTMs outputs and hidden units (h, c)\n",
        "        \"\"\"\n",
        "\n",
        "        embedded_inputs = embedded_inputs.permute(1, 0, 2)\n",
        "\n",
        "        outputs, hidden = self.lstm(embedded_inputs, hidden)\n",
        "\n",
        "        return outputs.permute(1, 0, 2), hidden\n",
        "\n",
        "    def init_hidden(self, embedded_inputs):\n",
        "        \"\"\"\n",
        "        Initiate hidden units\n",
        "\n",
        "        :param Tensor embedded_inputs: The embedded input of Pointer-NEt\n",
        "        :return: Initiated hidden units for the LSTMs (h, c)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = embedded_inputs.size(0)\n",
        "\n",
        "        # Reshaping (Expanding)\n",
        "        h0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
        "                                                      batch_size,\n",
        "                                                      self.hidden_dim)\n",
        "        c0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
        "                                                      batch_size,\n",
        "                                                      self.hidden_dim)\n",
        "\n",
        "        return h0, c0\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention model for Pointer-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim,\n",
        "                 hidden_dim):\n",
        "        \"\"\"\n",
        "        Initiate Attention\n",
        "\n",
        "        :param int input_dim: Input's diamention\n",
        "        :param int hidden_dim: Number of hidden units in the attention\n",
        "        \"\"\"\n",
        "\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.context_linear = nn.Conv1d(input_dim, hidden_dim, 1, 1)\n",
        "        self.V = Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
        "        self._inf = Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        # Initialize vector V\n",
        "        nn.init.uniform(self.V, -1, 1)\n",
        "\n",
        "    def forward(self, input,\n",
        "                context,\n",
        "                mask):\n",
        "        \"\"\"\n",
        "        Attention - Forward-pass\n",
        "\n",
        "        :param Tensor input: Hidden state h\n",
        "        :param Tensor context: Attention context\n",
        "        :param ByteTensor mask: Selection mask\n",
        "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
        "        \"\"\"\n",
        "\n",
        "        # (batch, hidden_dim, seq_len)\n",
        "        inp = self.input_linear(input).unsqueeze(2).expand(-1, -1, context.size(1))\n",
        "\n",
        "        # (batch, hidden_dim, seq_len)\n",
        "        context = context.permute(0, 2, 1)\n",
        "        ctx = self.context_linear(context)\n",
        "\n",
        "        # (batch, 1, hidden_dim)\n",
        "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
        "\n",
        "        # (batch, seq_len)\n",
        "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
        "        if len(att[mask]) > 0:\n",
        "            att[mask] = self.inf[mask]\n",
        "        alpha = self.softmax(att)\n",
        "\n",
        "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        return hidden_state, alpha\n",
        "\n",
        "    def init_inf(self, mask_size):\n",
        "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder model for Pointer-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim,\n",
        "                 hidden_dim):\n",
        "        \"\"\"\n",
        "        Initiate Decoder\n",
        "\n",
        "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
        "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
        "        \"\"\"\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
        "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
        "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.att = Attention(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Used for propagating .cuda() command\n",
        "        self.mask = Parameter(torch.ones(1), requires_grad=False)\n",
        "        self.runner = Parameter(torch.zeros(1), requires_grad=False)\n",
        "\n",
        "    def forward(self, embedded_inputs,\n",
        "                decoder_input,\n",
        "                hidden,\n",
        "                context):\n",
        "        \"\"\"\n",
        "        Decoder - Forward-pass\n",
        "\n",
        "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
        "        :param Tensor decoder_input: First decoder's input\n",
        "        :param Tensor hidden: First decoder's hidden states\n",
        "        :param Tensor context: Encoder's outputs\n",
        "        :return: (Output probabilities, Pointers indices), last hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = embedded_inputs.size(0)\n",
        "        input_length = embedded_inputs.size(1)\n",
        "\n",
        "        # (batch, seq_len)\n",
        "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
        "        self.att.init_inf(mask.size())\n",
        "\n",
        "        # Generating arang(input_length), broadcasted across batch_size\n",
        "        runner = self.runner.repeat(input_length)\n",
        "        for i in range(input_length):\n",
        "            runner.data[i] = i\n",
        "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
        "\n",
        "        outputs = []\n",
        "        pointers = []\n",
        "\n",
        "        def step(x, hidden):\n",
        "            \"\"\"\n",
        "            Recurrence step function\n",
        "\n",
        "            :param Tensor x: Input at time t\n",
        "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
        "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
        "            \"\"\"\n",
        "\n",
        "            # Regular LSTM\n",
        "            h, c = hidden\n",
        "\n",
        "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h)\n",
        "            input, forget, cell, out = gates.chunk(4, 1)\n",
        "\n",
        "            input = F.sigmoid(input)\n",
        "            forget = F.sigmoid(forget)\n",
        "            cell = F.tanh(cell)\n",
        "            out = F.sigmoid(out)\n",
        "\n",
        "            c_t = (forget * c) + (input * cell)\n",
        "            h_t = out * F.tanh(c_t)\n",
        "\n",
        "            # Attention section\n",
        "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
        "            hidden_t = F.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
        "\n",
        "            return hidden_t, c_t, output\n",
        "\n",
        "        # Recurrence loop\n",
        "        for _ in range(input_length):\n",
        "            h_t, c_t, outs = step(decoder_input, hidden)\n",
        "            hidden = (h_t, c_t)\n",
        "\n",
        "            # Masking selected inputs\n",
        "            masked_outs = outs * mask\n",
        "\n",
        "            # Get maximum probabilities and indices\n",
        "            max_probs, indices = masked_outs.max(1)\n",
        "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
        "\n",
        "            # Update mask to ignore seen indices\n",
        "            mask  = mask * (1 - one_hot_pointers)\n",
        "\n",
        "            # Get embedded inputs by max indices\n",
        "            embedding_mask = one_hot_pointers.unsqueeze(2).expand(-1, -1, self.embedding_dim).byte()\n",
        "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
        "\n",
        "            outputs.append(outs.unsqueeze(0))\n",
        "            pointers.append(indices.unsqueeze(1))\n",
        "\n",
        "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
        "        pointers = torch.cat(pointers, 1)\n",
        "\n",
        "        return (outputs, pointers), hidden\n",
        "\n",
        "\n",
        "class PointerNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Pointer-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 lstm_layers,\n",
        "                 dropout,\n",
        "                 bidir=False):\n",
        "        \"\"\"\n",
        "        Initiate Pointer-Net\n",
        "\n",
        "        :param int embedding_dim: Number of embbeding channels\n",
        "        :param int hidden_dim: Encoders hidden units\n",
        "        :param int lstm_layers: Number of layers for LSTMs\n",
        "        :param float dropout: Float between 0-1\n",
        "        :param bool bidir: Bidirectional\n",
        "        \"\"\"\n",
        "\n",
        "        super(PointerNet, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.bidir = bidir\n",
        "        self.embedding = nn.Linear(2, embedding_dim)\n",
        "        self.encoder = Encoder(embedding_dim,\n",
        "                               hidden_dim,\n",
        "                               lstm_layers,\n",
        "                               dropout,\n",
        "                               bidir)\n",
        "        self.decoder = Decoder(embedding_dim, hidden_dim)\n",
        "        self.decoder_input0 = Parameter(torch.FloatTensor(embedding_dim), requires_grad=False)\n",
        "\n",
        "        # Initialize decoder_input0\n",
        "        nn.init.uniform(self.decoder_input0, -1, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        PointerNet - Forward-pass\n",
        "\n",
        "        :param Tensor inputs: Input sequence\n",
        "        :return: Pointers probabilities and indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = inputs.size(0)\n",
        "        input_length = inputs.size(1)\n",
        "\n",
        "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        inputs = inputs.view(batch_size * input_length, -1)\n",
        "        embedded_inputs = self.embedding(inputs).view(batch_size, input_length, -1)\n",
        "\n",
        "        encoder_hidden0 = self.encoder.init_hidden(embedded_inputs)\n",
        "        encoder_outputs, encoder_hidden = self.encoder(embedded_inputs,\n",
        "                                                       encoder_hidden0)\n",
        "        if self.bidir:\n",
        "        # Concatenate the last two hidden states of the encoder\n",
        "          hidden_0 = torch.cat((encoder_hidden[0][-2], encoder_hidden[0][-1]), dim=-1)\n",
        "          hidden_1 = torch.cat((encoder_hidden[1][-2], encoder_hidden[1][-1]), dim=-1)\n",
        "          decoder_hidden0 = (hidden_0, hidden_1)\n",
        "        else:\n",
        "          decoder_hidden0 = (encoder_hidden[0][-1], encoder_hidden[1][-1])\n",
        "\n",
        "        (outputs, pointers), decoder_hidden = self.decoder(embedded_inputs,\n",
        "                                                           decoder_input0,\n",
        "                                                           decoder_hidden0,\n",
        "                                                           encoder_outputs)\n",
        "\n",
        "        return  outputs, pointers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Define training parameters\n",
        "params = {\n",
        "    'train_size': 1000,\n",
        "    'val_size': 100,\n",
        "    'test_size': 100,\n",
        "    'batch_size': 256,\n",
        "    'nof_epoch': 500,\n",
        "    'lr': 0.0001,\n",
        "    'gpu': True,\n",
        "    'nof_points': 5,\n",
        "    'embedding_size': 128,\n",
        "    'hiddens': 512,\n",
        "    'nof_lstms': 2,\n",
        "    'dropout': 0.,\n",
        "    'bidir': True\n",
        "}\n",
        "\n",
        "# Check if GPU is available\n",
        "if params['gpu'] and torch.cuda.is_available():\n",
        "    USE_CUDA = True\n",
        "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
        "else:\n",
        "    USE_CUDA = False\n",
        "\n",
        "# Initialize the model\n",
        "model = PointerNet(params['embedding_size'],\n",
        "                   params['hiddens'],\n",
        "                   params['nof_lstms'],\n",
        "                   params['dropout'],\n",
        "                   params['bidir'])\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = TSPDataset(params['train_size'],\n",
        "                     params['nof_points'])\n",
        "\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        shuffle=True,\n",
        "                        num_workers=4)\n",
        "\n",
        "# Move model to GPU if available\n",
        "if USE_CUDA:\n",
        "    model.cuda()\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "CCE = torch.nn.CrossEntropyLoss()\n",
        "model_optim = optim.Adam(filter(lambda p: p.requires_grad,\n",
        "                                model.parameters()),\n",
        "                         lr=params['lr'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(params['nof_epoch']):\n",
        "    batch_loss = []\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(dataloader):\n",
        "        train_batch = Variable(sample_batched['Points'])\n",
        "        target_batch = Variable(sample_batched['Solution'])\n",
        "\n",
        "        if USE_CUDA:\n",
        "            train_batch = train_batch.cuda()\n",
        "            target_batch = target_batch.cuda()\n",
        "\n",
        "        o, p = model(train_batch)\n",
        "        o = o.contiguous().view(-1, o.size()[-1])\n",
        "\n",
        "        target_batch = target_batch.view(-1)\n",
        "\n",
        "        loss = CCE(o, target_batch)\n",
        "\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "        model_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        model_optim.step()\n",
        "\n",
        "    average_loss = sum(batch_loss) / len(batch_loss)\n",
        "    print(f'Epoch {epoch+1}/{params[\"nof_epoch\"]}, Average Loss: {average_loss:.4f}')\n",
        "\n",
        "# Save the trained model if needed\n",
        "# torch.save(model.state_dict(), 'pointer_net_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTcsODMPQ8kL",
        "outputId": "7c8bbcad-d5f8-4846-80d6-f72d2d7cb68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-09d66fb904e7>:105: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(self.V, -1, 1)\n",
            "<ipython-input-5-09d66fb904e7>:290: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(self.decoder_input0, -1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU, 1 devices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data points 1000/1000: 100%|██████████| 1000/1000 [00:01<00:00, 905.52data/s]\n",
            "Solved 1000/1000: 100%|██████████| 1000/1000 [00:01<00:00, 628.07solve/s]\n",
            "<ipython-input-5-09d66fb904e7>:246: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Average Loss: 1.5319\n",
            "Epoch 2/500, Average Loss: 1.4689\n",
            "Epoch 3/500, Average Loss: 1.4379\n",
            "Epoch 4/500, Average Loss: 1.4287\n",
            "Epoch 5/500, Average Loss: 1.4246\n",
            "Epoch 6/500, Average Loss: 1.4354\n",
            "Epoch 7/500, Average Loss: 1.4378\n",
            "Epoch 8/500, Average Loss: 1.4366\n",
            "Epoch 9/500, Average Loss: 1.4410\n",
            "Epoch 10/500, Average Loss: 1.4413\n",
            "Epoch 11/500, Average Loss: 1.4418\n",
            "Epoch 12/500, Average Loss: 1.4397\n",
            "Epoch 13/500, Average Loss: 1.4363\n",
            "Epoch 14/500, Average Loss: 1.4369\n",
            "Epoch 15/500, Average Loss: 1.4385\n",
            "Epoch 16/500, Average Loss: 1.4387\n",
            "Epoch 17/500, Average Loss: 1.4384\n",
            "Epoch 18/500, Average Loss: 1.4380\n",
            "Epoch 19/500, Average Loss: 1.4362\n",
            "Epoch 20/500, Average Loss: 1.4357\n",
            "Epoch 21/500, Average Loss: 1.4352\n",
            "Epoch 22/500, Average Loss: 1.4367\n",
            "Epoch 23/500, Average Loss: 1.4368\n",
            "Epoch 24/500, Average Loss: 1.4343\n",
            "Epoch 25/500, Average Loss: 1.4341\n",
            "Epoch 26/500, Average Loss: 1.4359\n",
            "Epoch 27/500, Average Loss: 1.4354\n",
            "Epoch 28/500, Average Loss: 1.4349\n",
            "Epoch 29/500, Average Loss: 1.4347\n",
            "Epoch 30/500, Average Loss: 1.4339\n",
            "Epoch 31/500, Average Loss: 1.4331\n",
            "Epoch 32/500, Average Loss: 1.4323\n",
            "Epoch 33/500, Average Loss: 1.4329\n",
            "Epoch 34/500, Average Loss: 1.4303\n",
            "Epoch 35/500, Average Loss: 1.4298\n",
            "Epoch 36/500, Average Loss: 1.4302\n",
            "Epoch 37/500, Average Loss: 1.4306\n",
            "Epoch 38/500, Average Loss: 1.4288\n",
            "Epoch 39/500, Average Loss: 1.4262\n",
            "Epoch 40/500, Average Loss: 1.4281\n",
            "Epoch 41/500, Average Loss: 1.4289\n",
            "Epoch 42/500, Average Loss: 1.4281\n",
            "Epoch 43/500, Average Loss: 1.4283\n",
            "Epoch 44/500, Average Loss: 1.4273\n",
            "Epoch 45/500, Average Loss: 1.4290\n",
            "Epoch 46/500, Average Loss: 1.4259\n",
            "Epoch 47/500, Average Loss: 1.4249\n",
            "Epoch 48/500, Average Loss: 1.4263\n",
            "Epoch 49/500, Average Loss: 1.4250\n",
            "Epoch 50/500, Average Loss: 1.4250\n",
            "Epoch 51/500, Average Loss: 1.4235\n",
            "Epoch 52/500, Average Loss: 1.4234\n",
            "Epoch 53/500, Average Loss: 1.4247\n",
            "Epoch 54/500, Average Loss: 1.4214\n",
            "Epoch 55/500, Average Loss: 1.4234\n",
            "Epoch 56/500, Average Loss: 1.4235\n",
            "Epoch 57/500, Average Loss: 1.4184\n",
            "Epoch 58/500, Average Loss: 1.4144\n",
            "Epoch 59/500, Average Loss: 1.4215\n",
            "Epoch 60/500, Average Loss: 1.4182\n",
            "Epoch 61/500, Average Loss: 1.4132\n",
            "Epoch 62/500, Average Loss: 1.4102\n",
            "Epoch 63/500, Average Loss: 1.4073\n",
            "Epoch 64/500, Average Loss: 1.4025\n",
            "Epoch 65/500, Average Loss: 1.3968\n",
            "Epoch 66/500, Average Loss: 1.3932\n",
            "Epoch 67/500, Average Loss: 1.3906\n",
            "Epoch 68/500, Average Loss: 1.3839\n",
            "Epoch 69/500, Average Loss: 1.3802\n",
            "Epoch 70/500, Average Loss: 1.3703\n",
            "Epoch 71/500, Average Loss: 1.3688\n",
            "Epoch 72/500, Average Loss: 1.3739\n",
            "Epoch 73/500, Average Loss: 1.3705\n",
            "Epoch 74/500, Average Loss: 1.3636\n",
            "Epoch 75/500, Average Loss: 1.3580\n",
            "Epoch 76/500, Average Loss: 1.3543\n",
            "Epoch 77/500, Average Loss: 1.3546\n",
            "Epoch 78/500, Average Loss: 1.3489\n",
            "Epoch 79/500, Average Loss: 1.3507\n",
            "Epoch 80/500, Average Loss: 1.3432\n",
            "Epoch 81/500, Average Loss: 1.3457\n",
            "Epoch 82/500, Average Loss: 1.3417\n",
            "Epoch 83/500, Average Loss: 1.3408\n",
            "Epoch 84/500, Average Loss: 1.3389\n",
            "Epoch 85/500, Average Loss: 1.3452\n",
            "Epoch 86/500, Average Loss: 1.3471\n",
            "Epoch 87/500, Average Loss: 1.3387\n",
            "Epoch 88/500, Average Loss: 1.3515\n",
            "Epoch 89/500, Average Loss: 1.3380\n",
            "Epoch 90/500, Average Loss: 1.3382\n",
            "Epoch 91/500, Average Loss: 1.3533\n",
            "Epoch 92/500, Average Loss: 1.3325\n",
            "Epoch 93/500, Average Loss: 1.3352\n",
            "Epoch 94/500, Average Loss: 1.3378\n",
            "Epoch 95/500, Average Loss: 1.3350\n",
            "Epoch 96/500, Average Loss: 1.3298\n",
            "Epoch 97/500, Average Loss: 1.3273\n",
            "Epoch 98/500, Average Loss: 1.3276\n",
            "Epoch 99/500, Average Loss: 1.3276\n",
            "Epoch 100/500, Average Loss: 1.3213\n",
            "Epoch 101/500, Average Loss: 1.3301\n",
            "Epoch 102/500, Average Loss: 1.3211\n",
            "Epoch 103/500, Average Loss: 1.3194\n",
            "Epoch 104/500, Average Loss: 1.3219\n",
            "Epoch 105/500, Average Loss: 1.3140\n",
            "Epoch 106/500, Average Loss: 1.3152\n",
            "Epoch 107/500, Average Loss: 1.3103\n",
            "Epoch 108/500, Average Loss: 1.3144\n",
            "Epoch 109/500, Average Loss: 1.3117\n",
            "Epoch 110/500, Average Loss: 1.3094\n",
            "Epoch 111/500, Average Loss: 1.3071\n",
            "Epoch 112/500, Average Loss: 1.3108\n",
            "Epoch 113/500, Average Loss: 1.3044\n",
            "Epoch 114/500, Average Loss: 1.3035\n",
            "Epoch 115/500, Average Loss: 1.3040\n",
            "Epoch 116/500, Average Loss: 1.3097\n",
            "Epoch 117/500, Average Loss: 1.3056\n",
            "Epoch 118/500, Average Loss: 1.2991\n",
            "Epoch 119/500, Average Loss: 1.3014\n",
            "Epoch 120/500, Average Loss: 1.2957\n",
            "Epoch 121/500, Average Loss: 1.3125\n",
            "Epoch 122/500, Average Loss: 1.3004\n",
            "Epoch 123/500, Average Loss: 1.2948\n",
            "Epoch 124/500, Average Loss: 1.2955\n",
            "Epoch 125/500, Average Loss: 1.2990\n",
            "Epoch 126/500, Average Loss: 1.2886\n",
            "Epoch 127/500, Average Loss: 1.2894\n",
            "Epoch 128/500, Average Loss: 1.2851\n",
            "Epoch 129/500, Average Loss: 1.2899\n",
            "Epoch 130/500, Average Loss: 1.2818\n",
            "Epoch 131/500, Average Loss: 1.2833\n",
            "Epoch 132/500, Average Loss: 1.2817\n",
            "Epoch 133/500, Average Loss: 1.2842\n",
            "Epoch 134/500, Average Loss: 1.2761\n",
            "Epoch 135/500, Average Loss: 1.2835\n",
            "Epoch 136/500, Average Loss: 1.2764\n",
            "Epoch 137/500, Average Loss: 1.2778\n",
            "Epoch 138/500, Average Loss: 1.2758\n",
            "Epoch 139/500, Average Loss: 1.2732\n",
            "Epoch 140/500, Average Loss: 1.2768\n",
            "Epoch 141/500, Average Loss: 1.2697\n",
            "Epoch 142/500, Average Loss: 1.2686\n",
            "Epoch 143/500, Average Loss: 1.2721\n",
            "Epoch 144/500, Average Loss: 1.2661\n",
            "Epoch 145/500, Average Loss: 1.2734\n",
            "Epoch 146/500, Average Loss: 1.2638\n",
            "Epoch 147/500, Average Loss: 1.2587\n",
            "Epoch 148/500, Average Loss: 1.2540\n",
            "Epoch 149/500, Average Loss: 1.2579\n",
            "Epoch 150/500, Average Loss: 1.2545\n",
            "Epoch 151/500, Average Loss: 1.2499\n",
            "Epoch 152/500, Average Loss: 1.2539\n",
            "Epoch 153/500, Average Loss: 1.2451\n",
            "Epoch 154/500, Average Loss: 1.2512\n",
            "Epoch 155/500, Average Loss: 1.2409\n",
            "Epoch 156/500, Average Loss: 1.2475\n",
            "Epoch 157/500, Average Loss: 1.2420\n",
            "Epoch 158/500, Average Loss: 1.2417\n",
            "Epoch 159/500, Average Loss: 1.2358\n",
            "Epoch 160/500, Average Loss: 1.2359\n",
            "Epoch 161/500, Average Loss: 1.2348\n",
            "Epoch 162/500, Average Loss: 1.2319\n",
            "Epoch 163/500, Average Loss: 1.2290\n",
            "Epoch 164/500, Average Loss: 1.2348\n",
            "Epoch 165/500, Average Loss: 1.2278\n",
            "Epoch 166/500, Average Loss: 1.2241\n",
            "Epoch 167/500, Average Loss: 1.2203\n",
            "Epoch 168/500, Average Loss: 1.2217\n",
            "Epoch 169/500, Average Loss: 1.2171\n",
            "Epoch 170/500, Average Loss: 1.2175\n",
            "Epoch 171/500, Average Loss: 1.2169\n",
            "Epoch 172/500, Average Loss: 1.2176\n",
            "Epoch 173/500, Average Loss: 1.2143\n",
            "Epoch 174/500, Average Loss: 1.2206\n",
            "Epoch 175/500, Average Loss: 1.2117\n",
            "Epoch 176/500, Average Loss: 1.2125\n",
            "Epoch 177/500, Average Loss: 1.2099\n",
            "Epoch 178/500, Average Loss: 1.2079\n",
            "Epoch 179/500, Average Loss: 1.2000\n",
            "Epoch 180/500, Average Loss: 1.1978\n",
            "Epoch 181/500, Average Loss: 1.1998\n",
            "Epoch 182/500, Average Loss: 1.2047\n",
            "Epoch 183/500, Average Loss: 1.2014\n",
            "Epoch 184/500, Average Loss: 1.1981\n",
            "Epoch 185/500, Average Loss: 1.1959\n",
            "Epoch 186/500, Average Loss: 1.1963\n",
            "Epoch 187/500, Average Loss: 1.1938\n",
            "Epoch 188/500, Average Loss: 1.1941\n",
            "Epoch 189/500, Average Loss: 1.1895\n",
            "Epoch 190/500, Average Loss: 1.1887\n",
            "Epoch 191/500, Average Loss: 1.1923\n",
            "Epoch 192/500, Average Loss: 1.1807\n",
            "Epoch 193/500, Average Loss: 1.1791\n",
            "Epoch 194/500, Average Loss: 1.1803\n",
            "Epoch 195/500, Average Loss: 1.1761\n",
            "Epoch 196/500, Average Loss: 1.1751\n",
            "Epoch 197/500, Average Loss: 1.1731\n",
            "Epoch 198/500, Average Loss: 1.1730\n",
            "Epoch 199/500, Average Loss: 1.1699\n",
            "Epoch 200/500, Average Loss: 1.1717\n",
            "Epoch 201/500, Average Loss: 1.1695\n",
            "Epoch 202/500, Average Loss: 1.1666\n",
            "Epoch 203/500, Average Loss: 1.1656\n",
            "Epoch 204/500, Average Loss: 1.1621\n",
            "Epoch 205/500, Average Loss: 1.1588\n",
            "Epoch 206/500, Average Loss: 1.1566\n",
            "Epoch 207/500, Average Loss: 1.1547\n",
            "Epoch 208/500, Average Loss: 1.1539\n",
            "Epoch 209/500, Average Loss: 1.1535\n",
            "Epoch 210/500, Average Loss: 1.1520\n",
            "Epoch 211/500, Average Loss: 1.1525\n",
            "Epoch 212/500, Average Loss: 1.1503\n",
            "Epoch 213/500, Average Loss: 1.1497\n",
            "Epoch 214/500, Average Loss: 1.1493\n",
            "Epoch 215/500, Average Loss: 1.1483\n",
            "Epoch 216/500, Average Loss: 1.1457\n",
            "Epoch 217/500, Average Loss: 1.1450\n",
            "Epoch 218/500, Average Loss: 1.1439\n",
            "Epoch 219/500, Average Loss: 1.1431\n",
            "Epoch 220/500, Average Loss: 1.1427\n",
            "Epoch 221/500, Average Loss: 1.1423\n",
            "Epoch 222/500, Average Loss: 1.1404\n",
            "Epoch 223/500, Average Loss: 1.1393\n",
            "Epoch 224/500, Average Loss: 1.1387\n",
            "Epoch 225/500, Average Loss: 1.1361\n",
            "Epoch 226/500, Average Loss: 1.1351\n",
            "Epoch 227/500, Average Loss: 1.1330\n",
            "Epoch 228/500, Average Loss: 1.1317\n",
            "Epoch 229/500, Average Loss: 1.1308\n",
            "Epoch 230/500, Average Loss: 1.1301\n",
            "Epoch 231/500, Average Loss: 1.1277\n",
            "Epoch 232/500, Average Loss: 1.1283\n",
            "Epoch 233/500, Average Loss: 1.1253\n",
            "Epoch 234/500, Average Loss: 1.1239\n",
            "Epoch 235/500, Average Loss: 1.1235\n",
            "Epoch 236/500, Average Loss: 1.1213\n",
            "Epoch 237/500, Average Loss: 1.1223\n",
            "Epoch 238/500, Average Loss: 1.1191\n",
            "Epoch 239/500, Average Loss: 1.1215\n",
            "Epoch 240/500, Average Loss: 1.1186\n",
            "Epoch 241/500, Average Loss: 1.1185\n",
            "Epoch 242/500, Average Loss: 1.1185\n",
            "Epoch 243/500, Average Loss: 1.1172\n",
            "Epoch 244/500, Average Loss: 1.1192\n",
            "Epoch 245/500, Average Loss: 1.1200\n",
            "Epoch 246/500, Average Loss: 1.1157\n",
            "Epoch 247/500, Average Loss: 1.1170\n",
            "Epoch 248/500, Average Loss: 1.1187\n",
            "Epoch 249/500, Average Loss: 1.1182\n",
            "Epoch 250/500, Average Loss: 1.1169\n",
            "Epoch 251/500, Average Loss: 1.1170\n",
            "Epoch 252/500, Average Loss: 1.1159\n",
            "Epoch 253/500, Average Loss: 1.1274\n",
            "Epoch 254/500, Average Loss: 1.1320\n",
            "Epoch 255/500, Average Loss: 1.1206\n",
            "Epoch 256/500, Average Loss: 1.1350\n",
            "Epoch 257/500, Average Loss: 1.1208\n",
            "Epoch 258/500, Average Loss: 1.1198\n",
            "Epoch 259/500, Average Loss: 1.1178\n",
            "Epoch 260/500, Average Loss: 1.1148\n",
            "Epoch 261/500, Average Loss: 1.1140\n",
            "Epoch 262/500, Average Loss: 1.1148\n",
            "Epoch 263/500, Average Loss: 1.1127\n",
            "Epoch 264/500, Average Loss: 1.1149\n",
            "Epoch 265/500, Average Loss: 1.1115\n",
            "Epoch 266/500, Average Loss: 1.1067\n",
            "Epoch 267/500, Average Loss: 1.1104\n",
            "Epoch 268/500, Average Loss: 1.1085\n",
            "Epoch 269/500, Average Loss: 1.1074\n",
            "Epoch 270/500, Average Loss: 1.1060\n",
            "Epoch 271/500, Average Loss: 1.1054\n",
            "Epoch 272/500, Average Loss: 1.1050\n",
            "Epoch 273/500, Average Loss: 1.1033\n",
            "Epoch 274/500, Average Loss: 1.1041\n",
            "Epoch 275/500, Average Loss: 1.1036\n",
            "Epoch 276/500, Average Loss: 1.1033\n",
            "Epoch 277/500, Average Loss: 1.1012\n",
            "Epoch 278/500, Average Loss: 1.1015\n",
            "Epoch 279/500, Average Loss: 1.1012\n",
            "Epoch 280/500, Average Loss: 1.1008\n",
            "Epoch 281/500, Average Loss: 1.1011\n",
            "Epoch 282/500, Average Loss: 1.1007\n",
            "Epoch 283/500, Average Loss: 1.1008\n",
            "Epoch 284/500, Average Loss: 1.1001\n",
            "Epoch 285/500, Average Loss: 1.0993\n",
            "Epoch 286/500, Average Loss: 1.0998\n",
            "Epoch 287/500, Average Loss: 1.1000\n",
            "Epoch 288/500, Average Loss: 1.0984\n",
            "Epoch 289/500, Average Loss: 1.0973\n",
            "Epoch 290/500, Average Loss: 1.0968\n",
            "Epoch 291/500, Average Loss: 1.0963\n",
            "Epoch 292/500, Average Loss: 1.0963\n",
            "Epoch 293/500, Average Loss: 1.0954\n",
            "Epoch 294/500, Average Loss: 1.0945\n",
            "Epoch 295/500, Average Loss: 1.0944\n",
            "Epoch 296/500, Average Loss: 1.0947\n",
            "Epoch 297/500, Average Loss: 1.0947\n",
            "Epoch 298/500, Average Loss: 1.0938\n",
            "Epoch 299/500, Average Loss: 1.0936\n",
            "Epoch 300/500, Average Loss: 1.0938\n",
            "Epoch 301/500, Average Loss: 1.0928\n",
            "Epoch 302/500, Average Loss: 1.0932\n",
            "Epoch 303/500, Average Loss: 1.0940\n",
            "Epoch 304/500, Average Loss: 1.0953\n",
            "Epoch 305/500, Average Loss: 1.0926\n",
            "Epoch 306/500, Average Loss: 1.0924\n",
            "Epoch 307/500, Average Loss: 1.0938\n",
            "Epoch 308/500, Average Loss: 1.0917\n",
            "Epoch 309/500, Average Loss: 1.0920\n",
            "Epoch 310/500, Average Loss: 1.0930\n",
            "Epoch 311/500, Average Loss: 1.0914\n",
            "Epoch 312/500, Average Loss: 1.0899\n",
            "Epoch 313/500, Average Loss: 1.0917\n",
            "Epoch 314/500, Average Loss: 1.0923\n",
            "Epoch 315/500, Average Loss: 1.0896\n",
            "Epoch 316/500, Average Loss: 1.0900\n",
            "Epoch 317/500, Average Loss: 1.0924\n",
            "Epoch 318/500, Average Loss: 1.0914\n",
            "Epoch 319/500, Average Loss: 1.0912\n",
            "Epoch 320/500, Average Loss: 1.0902\n",
            "Epoch 321/500, Average Loss: 1.0916\n",
            "Epoch 322/500, Average Loss: 1.0893\n",
            "Epoch 323/500, Average Loss: 1.0882\n",
            "Epoch 324/500, Average Loss: 1.0895\n",
            "Epoch 325/500, Average Loss: 1.0875\n",
            "Epoch 326/500, Average Loss: 1.0905\n",
            "Epoch 327/500, Average Loss: 1.0905\n",
            "Epoch 328/500, Average Loss: 1.0889\n",
            "Epoch 329/500, Average Loss: 1.0914\n",
            "Epoch 330/500, Average Loss: 1.0911\n",
            "Epoch 331/500, Average Loss: 1.0936\n",
            "Epoch 332/500, Average Loss: 1.0941\n",
            "Epoch 333/500, Average Loss: 1.0978\n",
            "Epoch 334/500, Average Loss: 1.1103\n",
            "Epoch 335/500, Average Loss: 1.1007\n",
            "Epoch 336/500, Average Loss: 1.1024\n",
            "Epoch 337/500, Average Loss: 1.1143\n",
            "Epoch 338/500, Average Loss: 1.0957\n",
            "Epoch 339/500, Average Loss: 1.0964\n",
            "Epoch 340/500, Average Loss: 1.0913\n",
            "Epoch 341/500, Average Loss: 1.0952\n",
            "Epoch 342/500, Average Loss: 1.0868\n",
            "Epoch 343/500, Average Loss: 1.0879\n",
            "Epoch 344/500, Average Loss: 1.0880\n",
            "Epoch 345/500, Average Loss: 1.0861\n",
            "Epoch 346/500, Average Loss: 1.0836\n",
            "Epoch 347/500, Average Loss: 1.0845\n",
            "Epoch 348/500, Average Loss: 1.0855\n",
            "Epoch 349/500, Average Loss: 1.0810\n",
            "Epoch 350/500, Average Loss: 1.0827\n",
            "Epoch 351/500, Average Loss: 1.0835\n",
            "Epoch 352/500, Average Loss: 1.0816\n",
            "Epoch 353/500, Average Loss: 1.0811\n",
            "Epoch 354/500, Average Loss: 1.0794\n",
            "Epoch 355/500, Average Loss: 1.0807\n",
            "Epoch 356/500, Average Loss: 1.0797\n",
            "Epoch 357/500, Average Loss: 1.0778\n",
            "Epoch 358/500, Average Loss: 1.0784\n",
            "Epoch 359/500, Average Loss: 1.0796\n",
            "Epoch 360/500, Average Loss: 1.0790\n",
            "Epoch 361/500, Average Loss: 1.0817\n",
            "Epoch 362/500, Average Loss: 1.0815\n",
            "Epoch 363/500, Average Loss: 1.0805\n",
            "Epoch 364/500, Average Loss: 1.0781\n",
            "Epoch 365/500, Average Loss: 1.0781\n",
            "Epoch 366/500, Average Loss: 1.0793\n",
            "Epoch 367/500, Average Loss: 1.0794\n",
            "Epoch 368/500, Average Loss: 1.0794\n",
            "Epoch 369/500, Average Loss: 1.0780\n",
            "Epoch 370/500, Average Loss: 1.0781\n",
            "Epoch 371/500, Average Loss: 1.0784\n",
            "Epoch 372/500, Average Loss: 1.0781\n",
            "Epoch 373/500, Average Loss: 1.0780\n",
            "Epoch 374/500, Average Loss: 1.0776\n",
            "Epoch 375/500, Average Loss: 1.0779\n",
            "Epoch 376/500, Average Loss: 1.0774\n",
            "Epoch 377/500, Average Loss: 1.0767\n",
            "Epoch 378/500, Average Loss: 1.0758\n",
            "Epoch 379/500, Average Loss: 1.0765\n",
            "Epoch 380/500, Average Loss: 1.0762\n",
            "Epoch 381/500, Average Loss: 1.0746\n",
            "Epoch 382/500, Average Loss: 1.0765\n",
            "Epoch 383/500, Average Loss: 1.0746\n",
            "Epoch 384/500, Average Loss: 1.0737\n",
            "Epoch 385/500, Average Loss: 1.0755\n",
            "Epoch 386/500, Average Loss: 1.0758\n",
            "Epoch 387/500, Average Loss: 1.0756\n",
            "Epoch 388/500, Average Loss: 1.0749\n",
            "Epoch 389/500, Average Loss: 1.0759\n",
            "Epoch 390/500, Average Loss: 1.0740\n",
            "Epoch 391/500, Average Loss: 1.0755\n",
            "Epoch 392/500, Average Loss: 1.0736\n",
            "Epoch 393/500, Average Loss: 1.0738\n",
            "Epoch 394/500, Average Loss: 1.0734\n",
            "Epoch 395/500, Average Loss: 1.0748\n",
            "Epoch 396/500, Average Loss: 1.0751\n",
            "Epoch 397/500, Average Loss: 1.0732\n",
            "Epoch 398/500, Average Loss: 1.0715\n",
            "Epoch 399/500, Average Loss: 1.0717\n",
            "Epoch 400/500, Average Loss: 1.0729\n",
            "Epoch 401/500, Average Loss: 1.0730\n",
            "Epoch 402/500, Average Loss: 1.0739\n",
            "Epoch 403/500, Average Loss: 1.0739\n",
            "Epoch 404/500, Average Loss: 1.0738\n",
            "Epoch 405/500, Average Loss: 1.0735\n",
            "Epoch 406/500, Average Loss: 1.0731\n",
            "Epoch 407/500, Average Loss: 1.0702\n",
            "Epoch 408/500, Average Loss: 1.0711\n",
            "Epoch 409/500, Average Loss: 1.0735\n",
            "Epoch 410/500, Average Loss: 1.0717\n",
            "Epoch 411/500, Average Loss: 1.0722\n",
            "Epoch 412/500, Average Loss: 1.0764\n",
            "Epoch 413/500, Average Loss: 1.0727\n",
            "Epoch 414/500, Average Loss: 1.0736\n",
            "Epoch 415/500, Average Loss: 1.0717\n",
            "Epoch 416/500, Average Loss: 1.0723\n",
            "Epoch 417/500, Average Loss: 1.0736\n",
            "Epoch 418/500, Average Loss: 1.0730\n",
            "Epoch 419/500, Average Loss: 1.0700\n",
            "Epoch 420/500, Average Loss: 1.0741\n",
            "Epoch 421/500, Average Loss: 1.0740\n",
            "Epoch 422/500, Average Loss: 1.0716\n",
            "Epoch 423/500, Average Loss: 1.0730\n",
            "Epoch 424/500, Average Loss: 1.0753\n",
            "Epoch 425/500, Average Loss: 1.0728\n",
            "Epoch 426/500, Average Loss: 1.0704\n",
            "Epoch 427/500, Average Loss: 1.0711\n",
            "Epoch 428/500, Average Loss: 1.0706\n",
            "Epoch 429/500, Average Loss: 1.0705\n",
            "Epoch 430/500, Average Loss: 1.0685\n",
            "Epoch 431/500, Average Loss: 1.0698\n",
            "Epoch 432/500, Average Loss: 1.0691\n",
            "Epoch 433/500, Average Loss: 1.0686\n",
            "Epoch 434/500, Average Loss: 1.0678\n",
            "Epoch 435/500, Average Loss: 1.0697\n",
            "Epoch 436/500, Average Loss: 1.0693\n",
            "Epoch 437/500, Average Loss: 1.0684\n",
            "Epoch 438/500, Average Loss: 1.0684\n",
            "Epoch 439/500, Average Loss: 1.0684\n",
            "Epoch 440/500, Average Loss: 1.0697\n",
            "Epoch 441/500, Average Loss: 1.0704\n",
            "Epoch 442/500, Average Loss: 1.0687\n",
            "Epoch 443/500, Average Loss: 1.0679\n",
            "Epoch 444/500, Average Loss: 1.0749\n",
            "Epoch 445/500, Average Loss: 1.0720\n",
            "Epoch 446/500, Average Loss: 1.0865\n",
            "Epoch 447/500, Average Loss: 1.0780\n",
            "Epoch 448/500, Average Loss: 1.0734\n",
            "Epoch 449/500, Average Loss: 1.0688\n",
            "Epoch 450/500, Average Loss: 1.0704\n",
            "Epoch 451/500, Average Loss: 1.0696\n",
            "Epoch 452/500, Average Loss: 1.0681\n",
            "Epoch 453/500, Average Loss: 1.0717\n",
            "Epoch 454/500, Average Loss: 1.0679\n",
            "Epoch 455/500, Average Loss: 1.0687\n",
            "Epoch 456/500, Average Loss: 1.0658\n",
            "Epoch 457/500, Average Loss: 1.0648\n",
            "Epoch 458/500, Average Loss: 1.0658\n",
            "Epoch 459/500, Average Loss: 1.0663\n",
            "Epoch 460/500, Average Loss: 1.0636\n",
            "Epoch 461/500, Average Loss: 1.0629\n",
            "Epoch 462/500, Average Loss: 1.0632\n",
            "Epoch 463/500, Average Loss: 1.0617\n",
            "Epoch 464/500, Average Loss: 1.0609\n",
            "Epoch 465/500, Average Loss: 1.0601\n",
            "Epoch 466/500, Average Loss: 1.0608\n",
            "Epoch 467/500, Average Loss: 1.0593\n",
            "Epoch 468/500, Average Loss: 1.0569\n",
            "Epoch 469/500, Average Loss: 1.0586\n",
            "Epoch 470/500, Average Loss: 1.0591\n",
            "Epoch 471/500, Average Loss: 1.0591\n",
            "Epoch 472/500, Average Loss: 1.0565\n",
            "Epoch 473/500, Average Loss: 1.0577\n",
            "Epoch 474/500, Average Loss: 1.0575\n",
            "Epoch 475/500, Average Loss: 1.0579\n",
            "Epoch 476/500, Average Loss: 1.0582\n",
            "Epoch 477/500, Average Loss: 1.0561\n",
            "Epoch 478/500, Average Loss: 1.0562\n",
            "Epoch 479/500, Average Loss: 1.0559\n",
            "Epoch 480/500, Average Loss: 1.0703\n",
            "Epoch 481/500, Average Loss: 1.0647\n",
            "Epoch 482/500, Average Loss: 1.0711\n",
            "Epoch 483/500, Average Loss: 1.0684\n",
            "Epoch 484/500, Average Loss: 1.0660\n",
            "Epoch 485/500, Average Loss: 1.0645\n",
            "Epoch 486/500, Average Loss: 1.0671\n",
            "Epoch 487/500, Average Loss: 1.0621\n",
            "Epoch 488/500, Average Loss: 1.0599\n",
            "Epoch 489/500, Average Loss: 1.0589\n",
            "Epoch 490/500, Average Loss: 1.0569\n",
            "Epoch 491/500, Average Loss: 1.0560\n",
            "Epoch 492/500, Average Loss: 1.0555\n",
            "Epoch 493/500, Average Loss: 1.0540\n",
            "Epoch 494/500, Average Loss: 1.0541\n",
            "Epoch 495/500, Average Loss: 1.0542\n",
            "Epoch 496/500, Average Loss: 1.0539\n",
            "Epoch 497/500, Average Loss: 1.0537\n",
            "Epoch 498/500, Average Loss: 1.0526\n",
            "Epoch 499/500, Average Loss: 1.0524\n",
            "Epoch 500/500, Average Loss: 1.0523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHNecyPPkZJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define training parameters\n",
        "params = {\n",
        "    'train_size': 1000,\n",
        "    'val_size': 100,\n",
        "    'test_size': 100,\n",
        "    'batch_size': 256,\n",
        "    'nof_epoch': 500,\n",
        "    'lr': 0.0001,\n",
        "    'gpu': True,\n",
        "    'nof_points': 5,\n",
        "    'embedding_size': 128,\n",
        "    'hiddens': 512,\n",
        "    'nof_lstms': 2,\n",
        "    'dropout': 0.,\n",
        "    'bidir': True\n",
        "}\n",
        "\n",
        "# Check if GPU is available\n",
        "if params['gpu'] and torch.cuda.is_available():\n",
        "    USE_CUDA = True\n",
        "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
        "else:\n",
        "    USE_CUDA = False\n",
        "\n",
        "# Initialize the model\n",
        "model = PointerNet(params['embedding_size'],\n",
        "                   params['hiddens'],\n",
        "                   params['nof_lstms'],\n",
        "                   params['dropout'],\n",
        "                   params['bidir'])\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = TSPDataset(params['train_size'],\n",
        "                     params['nof_points'])\n",
        "\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        shuffle=True,\n",
        "                        num_workers=4)\n",
        "\n",
        "# Move model to GPU if available\n",
        "if USE_CUDA:\n",
        "    model.cuda()\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "CCE = torch.nn.CrossEntropyLoss()\n",
        "model_optim = optim.Adam(filter(lambda p: p.requires_grad,\n",
        "                                model.parameters()),\n",
        "                         lr=params['lr'])\n",
        "\n",
        "# Initialize lists to store loss values\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(params['nof_epoch']):\n",
        "    batch_loss = []\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(dataloader):\n",
        "        train_batch = Variable(sample_batched['Points'])\n",
        "        target_batch = Variable(sample_batched['Solution'])\n",
        "\n",
        "        if USE_CUDA:\n",
        "            train_batch = train_batch.cuda()\n",
        "            target_batch = target_batch.cuda()\n",
        "\n",
        "        o, p = model(train_batch)\n",
        "        o = o.contiguous().view(-1, o.size()[-1])\n",
        "\n",
        "        target_batch = target_batch.view(-1)\n",
        "\n",
        "        loss = CCE(o, target_batch)\n",
        "\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "        model_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        model_optim.step()\n",
        "\n",
        "    average_loss = sum(batch_loss) / len(batch_loss)\n",
        "    print(f'Epoch {epoch+1}/{params[\"nof_epoch\"]}, Average Loss: {average_loss:.4f}')\n",
        "\n",
        "    # Store training loss for plotting\n",
        "    train_losses.append(average_loss)\n",
        "\n",
        "    # Optionally, calculate validation and test loss here if you have validation and test datasets\n",
        "    # val_loss = ...\n",
        "    # test_loss = ...\n",
        "    # val_losses.append(val_loss)\n",
        "    # test_losses.append(test_loss)\n",
        "\n",
        "# Plot the training, validation, and test losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, params['nof_epoch']+1), train_losses, label='Training Loss')\n",
        "# Optionally, plot validation and test losses if available\n",
        "# plt.plot(range(1, params['nof_epoch']+1), val_losses, label='Validation Loss')\n",
        "# plt.plot(range(1, params['nof_epoch']+1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model if needed\n",
        "# torch.save(model.state_dict(), 'pointer_net_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "116f8bbb-0fc1-4477-b494-a11ae7ea79fe",
        "id": "5TfyH-MyZbDl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-09d66fb904e7>:105: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(self.V, -1, 1)\n",
            "<ipython-input-5-09d66fb904e7>:290: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(self.decoder_input0, -1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU, 1 devices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data points 1000/1000: 100%|██████████| 1000/1000 [00:01<00:00, 933.62data/s]\n",
            "Solved 1000/1000: 100%|██████████| 1000/1000 [00:01<00:00, 647.18solve/s]\n",
            "<ipython-input-5-09d66fb904e7>:246: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Average Loss: 1.5347\n",
            "Epoch 2/500, Average Loss: 1.4657\n",
            "Epoch 3/500, Average Loss: 1.4348\n",
            "Epoch 4/500, Average Loss: 1.4277\n",
            "Epoch 5/500, Average Loss: 1.4326\n",
            "Epoch 6/500, Average Loss: 1.4405\n",
            "Epoch 7/500, Average Loss: 1.4455\n",
            "Epoch 8/500, Average Loss: 1.4473\n",
            "Epoch 9/500, Average Loss: 1.4469\n",
            "Epoch 10/500, Average Loss: 1.4435\n",
            "Epoch 11/500, Average Loss: 1.4398\n",
            "Epoch 12/500, Average Loss: 1.4392\n",
            "Epoch 13/500, Average Loss: 1.4401\n",
            "Epoch 14/500, Average Loss: 1.4392\n",
            "Epoch 15/500, Average Loss: 1.4379\n",
            "Epoch 16/500, Average Loss: 1.4323\n",
            "Epoch 17/500, Average Loss: 1.4239\n",
            "Epoch 18/500, Average Loss: 1.4138\n",
            "Epoch 19/500, Average Loss: 1.4124\n",
            "Epoch 20/500, Average Loss: 1.4118\n",
            "Epoch 21/500, Average Loss: 1.4121\n",
            "Epoch 22/500, Average Loss: 1.4118\n",
            "Epoch 23/500, Average Loss: 1.4118\n",
            "Epoch 24/500, Average Loss: 1.4116\n",
            "Epoch 25/500, Average Loss: 1.4116\n",
            "Epoch 26/500, Average Loss: 1.4114\n",
            "Epoch 27/500, Average Loss: 1.4109\n",
            "Epoch 28/500, Average Loss: 1.4109\n",
            "Epoch 29/500, Average Loss: 1.4109\n",
            "Epoch 30/500, Average Loss: 1.4110\n",
            "Epoch 31/500, Average Loss: 1.4111\n",
            "Epoch 32/500, Average Loss: 1.4110\n",
            "Epoch 33/500, Average Loss: 1.4107\n",
            "Epoch 34/500, Average Loss: 1.4106\n",
            "Epoch 35/500, Average Loss: 1.4103\n",
            "Epoch 36/500, Average Loss: 1.4107\n",
            "Epoch 37/500, Average Loss: 1.4102\n",
            "Epoch 38/500, Average Loss: 1.4105\n",
            "Epoch 39/500, Average Loss: 1.4100\n",
            "Epoch 40/500, Average Loss: 1.4100\n",
            "Epoch 41/500, Average Loss: 1.4102\n",
            "Epoch 42/500, Average Loss: 1.4115\n",
            "Epoch 43/500, Average Loss: 1.4221\n",
            "Epoch 44/500, Average Loss: 1.4312\n",
            "Epoch 45/500, Average Loss: 1.4459\n",
            "Epoch 46/500, Average Loss: 1.4419\n",
            "Epoch 47/500, Average Loss: 1.4312\n",
            "Epoch 48/500, Average Loss: 1.4459\n",
            "Epoch 49/500, Average Loss: 1.4510\n",
            "Epoch 50/500, Average Loss: 1.4340\n",
            "Epoch 51/500, Average Loss: 1.4350\n",
            "Epoch 52/500, Average Loss: 1.4407\n",
            "Epoch 53/500, Average Loss: 1.4290\n",
            "Epoch 54/500, Average Loss: 1.4345\n",
            "Epoch 55/500, Average Loss: 1.4422\n",
            "Epoch 56/500, Average Loss: 1.4378\n",
            "Epoch 57/500, Average Loss: 1.4321\n",
            "Epoch 58/500, Average Loss: 1.4280\n",
            "Epoch 59/500, Average Loss: 1.4376\n",
            "Epoch 60/500, Average Loss: 1.4370\n",
            "Epoch 61/500, Average Loss: 1.4294\n",
            "Epoch 62/500, Average Loss: 1.4246\n",
            "Epoch 63/500, Average Loss: 1.4166\n",
            "Epoch 64/500, Average Loss: 1.4241\n",
            "Epoch 65/500, Average Loss: 1.4323\n",
            "Epoch 66/500, Average Loss: 1.4239\n",
            "Epoch 67/500, Average Loss: 1.4170\n",
            "Epoch 68/500, Average Loss: 1.4182\n",
            "Epoch 69/500, Average Loss: 1.4148\n",
            "Epoch 70/500, Average Loss: 1.4147\n",
            "Epoch 71/500, Average Loss: 1.4111\n",
            "Epoch 72/500, Average Loss: 1.4065\n",
            "Epoch 73/500, Average Loss: 1.4047\n",
            "Epoch 74/500, Average Loss: 1.4000\n",
            "Epoch 75/500, Average Loss: 1.3996\n",
            "Epoch 76/500, Average Loss: 1.3983\n",
            "Epoch 77/500, Average Loss: 1.3983\n",
            "Epoch 78/500, Average Loss: 1.3975\n",
            "Epoch 79/500, Average Loss: 1.3964\n",
            "Epoch 80/500, Average Loss: 1.3978\n",
            "Epoch 81/500, Average Loss: 1.3975\n",
            "Epoch 82/500, Average Loss: 1.3952\n",
            "Epoch 83/500, Average Loss: 1.3972\n",
            "Epoch 84/500, Average Loss: 1.3965\n",
            "Epoch 85/500, Average Loss: 1.4002\n",
            "Epoch 86/500, Average Loss: 1.3953\n",
            "Epoch 87/500, Average Loss: 1.3933\n",
            "Epoch 88/500, Average Loss: 1.3947\n",
            "Epoch 89/500, Average Loss: 1.3934\n",
            "Epoch 90/500, Average Loss: 1.3919\n",
            "Epoch 91/500, Average Loss: 1.3910\n",
            "Epoch 92/500, Average Loss: 1.3899\n",
            "Epoch 93/500, Average Loss: 1.3841\n",
            "Epoch 94/500, Average Loss: 1.3824\n",
            "Epoch 95/500, Average Loss: 1.3866\n",
            "Epoch 96/500, Average Loss: 1.3780\n",
            "Epoch 97/500, Average Loss: 1.3790\n",
            "Epoch 98/500, Average Loss: 1.3740\n",
            "Epoch 99/500, Average Loss: 1.3746\n",
            "Epoch 100/500, Average Loss: 1.3696\n",
            "Epoch 101/500, Average Loss: 1.3686\n",
            "Epoch 102/500, Average Loss: 1.3644\n",
            "Epoch 103/500, Average Loss: 1.3650\n",
            "Epoch 104/500, Average Loss: 1.3640\n",
            "Epoch 105/500, Average Loss: 1.3614\n",
            "Epoch 106/500, Average Loss: 1.3601\n",
            "Epoch 107/500, Average Loss: 1.3604\n",
            "Epoch 108/500, Average Loss: 1.3544\n",
            "Epoch 109/500, Average Loss: 1.3539\n",
            "Epoch 110/500, Average Loss: 1.3542\n",
            "Epoch 111/500, Average Loss: 1.3535\n",
            "Epoch 112/500, Average Loss: 1.3452\n",
            "Epoch 113/500, Average Loss: 1.3527\n",
            "Epoch 114/500, Average Loss: 1.3528\n",
            "Epoch 115/500, Average Loss: 1.3436\n",
            "Epoch 116/500, Average Loss: 1.3547\n",
            "Epoch 117/500, Average Loss: 1.3373\n",
            "Epoch 118/500, Average Loss: 1.3367\n",
            "Epoch 119/500, Average Loss: 1.3418\n",
            "Epoch 120/500, Average Loss: 1.3326\n",
            "Epoch 121/500, Average Loss: 1.3320\n",
            "Epoch 122/500, Average Loss: 1.3367\n",
            "Epoch 123/500, Average Loss: 1.3264\n",
            "Epoch 124/500, Average Loss: 1.3289\n",
            "Epoch 125/500, Average Loss: 1.3280\n",
            "Epoch 126/500, Average Loss: 1.3202\n",
            "Epoch 127/500, Average Loss: 1.3186\n",
            "Epoch 128/500, Average Loss: 1.3221\n",
            "Epoch 129/500, Average Loss: 1.3089\n",
            "Epoch 130/500, Average Loss: 1.3068\n",
            "Epoch 131/500, Average Loss: 1.3052\n",
            "Epoch 132/500, Average Loss: 1.3056\n",
            "Epoch 133/500, Average Loss: 1.3021\n",
            "Epoch 134/500, Average Loss: 1.3041\n",
            "Epoch 135/500, Average Loss: 1.2922\n",
            "Epoch 136/500, Average Loss: 1.2947\n",
            "Epoch 137/500, Average Loss: 1.2907\n",
            "Epoch 138/500, Average Loss: 1.2901\n",
            "Epoch 139/500, Average Loss: 1.2846\n",
            "Epoch 140/500, Average Loss: 1.2786\n",
            "Epoch 141/500, Average Loss: 1.2791\n",
            "Epoch 142/500, Average Loss: 1.2733\n",
            "Epoch 143/500, Average Loss: 1.2711\n",
            "Epoch 144/500, Average Loss: 1.2656\n",
            "Epoch 145/500, Average Loss: 1.2729\n",
            "Epoch 146/500, Average Loss: 1.2604\n",
            "Epoch 147/500, Average Loss: 1.2572\n",
            "Epoch 148/500, Average Loss: 1.2547\n",
            "Epoch 149/500, Average Loss: 1.2573\n",
            "Epoch 150/500, Average Loss: 1.2502\n",
            "Epoch 151/500, Average Loss: 1.2414\n",
            "Epoch 152/500, Average Loss: 1.2324\n",
            "Epoch 153/500, Average Loss: 1.2312\n",
            "Epoch 154/500, Average Loss: 1.2277\n",
            "Epoch 155/500, Average Loss: 1.2207\n",
            "Epoch 156/500, Average Loss: 1.2145\n",
            "Epoch 157/500, Average Loss: 1.2172\n",
            "Epoch 158/500, Average Loss: 1.2135\n",
            "Epoch 159/500, Average Loss: 1.2046\n",
            "Epoch 160/500, Average Loss: 1.2039\n",
            "Epoch 161/500, Average Loss: 1.2013\n",
            "Epoch 162/500, Average Loss: 1.1921\n",
            "Epoch 163/500, Average Loss: 1.1890\n",
            "Epoch 164/500, Average Loss: 1.1851\n",
            "Epoch 165/500, Average Loss: 1.1783\n",
            "Epoch 166/500, Average Loss: 1.1785\n",
            "Epoch 167/500, Average Loss: 1.1744\n",
            "Epoch 168/500, Average Loss: 1.1729\n",
            "Epoch 169/500, Average Loss: 1.1644\n",
            "Epoch 170/500, Average Loss: 1.1671\n",
            "Epoch 171/500, Average Loss: 1.1624\n",
            "Epoch 172/500, Average Loss: 1.1666\n",
            "Epoch 173/500, Average Loss: 1.1642\n",
            "Epoch 174/500, Average Loss: 1.1647\n",
            "Epoch 175/500, Average Loss: 1.1561\n",
            "Epoch 176/500, Average Loss: 1.1526\n",
            "Epoch 177/500, Average Loss: 1.1488\n",
            "Epoch 178/500, Average Loss: 1.1458\n",
            "Epoch 179/500, Average Loss: 1.1490\n",
            "Epoch 180/500, Average Loss: 1.1415\n",
            "Epoch 181/500, Average Loss: 1.1359\n",
            "Epoch 182/500, Average Loss: 1.1342\n",
            "Epoch 183/500, Average Loss: 1.1389\n",
            "Epoch 184/500, Average Loss: 1.1336\n",
            "Epoch 185/500, Average Loss: 1.1340\n",
            "Epoch 186/500, Average Loss: 1.1246\n",
            "Epoch 187/500, Average Loss: 1.1245\n",
            "Epoch 188/500, Average Loss: 1.1221\n",
            "Epoch 189/500, Average Loss: 1.1209\n",
            "Epoch 190/500, Average Loss: 1.1182\n",
            "Epoch 191/500, Average Loss: 1.1140\n",
            "Epoch 192/500, Average Loss: 1.1093\n",
            "Epoch 193/500, Average Loss: 1.1065\n",
            "Epoch 194/500, Average Loss: 1.1078\n",
            "Epoch 195/500, Average Loss: 1.1043\n",
            "Epoch 196/500, Average Loss: 1.1010\n",
            "Epoch 197/500, Average Loss: 1.1022\n",
            "Epoch 198/500, Average Loss: 1.1002\n",
            "Epoch 199/500, Average Loss: 1.1005\n",
            "Epoch 200/500, Average Loss: 1.1006\n",
            "Epoch 201/500, Average Loss: 1.0999\n",
            "Epoch 202/500, Average Loss: 1.1018\n",
            "Epoch 203/500, Average Loss: 1.0979\n",
            "Epoch 204/500, Average Loss: 1.0958\n",
            "Epoch 205/500, Average Loss: 1.0938\n",
            "Epoch 206/500, Average Loss: 1.0924\n",
            "Epoch 207/500, Average Loss: 1.0926\n",
            "Epoch 208/500, Average Loss: 1.0903\n",
            "Epoch 209/500, Average Loss: 1.0881\n",
            "Epoch 210/500, Average Loss: 1.0899\n",
            "Epoch 211/500, Average Loss: 1.0880\n",
            "Epoch 212/500, Average Loss: 1.0874\n",
            "Epoch 213/500, Average Loss: 1.0857\n",
            "Epoch 214/500, Average Loss: 1.0839\n",
            "Epoch 215/500, Average Loss: 1.0849\n",
            "Epoch 216/500, Average Loss: 1.0825\n",
            "Epoch 217/500, Average Loss: 1.0819\n",
            "Epoch 218/500, Average Loss: 1.0823\n",
            "Epoch 219/500, Average Loss: 1.0814\n",
            "Epoch 220/500, Average Loss: 1.0808\n",
            "Epoch 221/500, Average Loss: 1.0803\n",
            "Epoch 222/500, Average Loss: 1.0792\n",
            "Epoch 223/500, Average Loss: 1.0774\n",
            "Epoch 224/500, Average Loss: 1.0778\n",
            "Epoch 225/500, Average Loss: 1.0759\n",
            "Epoch 226/500, Average Loss: 1.0757\n",
            "Epoch 227/500, Average Loss: 1.0760\n",
            "Epoch 228/500, Average Loss: 1.0769\n",
            "Epoch 229/500, Average Loss: 1.0752\n",
            "Epoch 230/500, Average Loss: 1.0733\n",
            "Epoch 231/500, Average Loss: 1.0738\n",
            "Epoch 232/500, Average Loss: 1.0749\n",
            "Epoch 233/500, Average Loss: 1.0744\n",
            "Epoch 234/500, Average Loss: 1.0731\n",
            "Epoch 235/500, Average Loss: 1.0703\n",
            "Epoch 236/500, Average Loss: 1.0705\n",
            "Epoch 237/500, Average Loss: 1.0690\n",
            "Epoch 238/500, Average Loss: 1.0690\n",
            "Epoch 239/500, Average Loss: 1.0713\n",
            "Epoch 240/500, Average Loss: 1.0708\n",
            "Epoch 241/500, Average Loss: 1.0864\n",
            "Epoch 242/500, Average Loss: 1.1426\n",
            "Epoch 243/500, Average Loss: 1.2227\n",
            "Epoch 244/500, Average Loss: 1.2066\n",
            "Epoch 245/500, Average Loss: 1.1794\n",
            "Epoch 246/500, Average Loss: 1.1826\n",
            "Epoch 247/500, Average Loss: 1.1302\n",
            "Epoch 248/500, Average Loss: 1.1150\n",
            "Epoch 249/500, Average Loss: 1.1015\n",
            "Epoch 250/500, Average Loss: 1.1135\n",
            "Epoch 251/500, Average Loss: 1.0941\n",
            "Epoch 252/500, Average Loss: 1.0989\n",
            "Epoch 253/500, Average Loss: 1.0989\n",
            "Epoch 254/500, Average Loss: 1.0848\n",
            "Epoch 255/500, Average Loss: 1.0882\n",
            "Epoch 256/500, Average Loss: 1.0997\n",
            "Epoch 257/500, Average Loss: 1.1139\n",
            "Epoch 258/500, Average Loss: 1.1006\n",
            "Epoch 259/500, Average Loss: 1.0911\n",
            "Epoch 260/500, Average Loss: 1.0852\n",
            "Epoch 261/500, Average Loss: 1.0857\n",
            "Epoch 262/500, Average Loss: 1.0776\n",
            "Epoch 263/500, Average Loss: 1.0780\n",
            "Epoch 264/500, Average Loss: 1.0779\n",
            "Epoch 265/500, Average Loss: 1.0719\n",
            "Epoch 266/500, Average Loss: 1.0722\n",
            "Epoch 267/500, Average Loss: 1.0680\n",
            "Epoch 268/500, Average Loss: 1.0650\n",
            "Epoch 269/500, Average Loss: 1.0621\n",
            "Epoch 270/500, Average Loss: 1.0603\n",
            "Epoch 271/500, Average Loss: 1.0583\n",
            "Epoch 272/500, Average Loss: 1.0591\n",
            "Epoch 273/500, Average Loss: 1.0582\n",
            "Epoch 274/500, Average Loss: 1.0563\n",
            "Epoch 275/500, Average Loss: 1.0575\n",
            "Epoch 276/500, Average Loss: 1.0603\n",
            "Epoch 277/500, Average Loss: 1.0595\n",
            "Epoch 278/500, Average Loss: 1.0919\n",
            "Epoch 279/500, Average Loss: 1.2079\n",
            "Epoch 280/500, Average Loss: 1.1977\n",
            "Epoch 281/500, Average Loss: 1.1651\n",
            "Epoch 282/500, Average Loss: 1.1481\n",
            "Epoch 283/500, Average Loss: 1.1280\n",
            "Epoch 284/500, Average Loss: 1.1110\n",
            "Epoch 285/500, Average Loss: 1.0924\n",
            "Epoch 286/500, Average Loss: 1.0818\n",
            "Epoch 287/500, Average Loss: 1.0840\n",
            "Epoch 288/500, Average Loss: 1.0689\n",
            "Epoch 289/500, Average Loss: 1.0675\n",
            "Epoch 290/500, Average Loss: 1.0627\n",
            "Epoch 291/500, Average Loss: 1.0591\n",
            "Epoch 292/500, Average Loss: 1.0587\n",
            "Epoch 293/500, Average Loss: 1.0582\n",
            "Epoch 294/500, Average Loss: 1.0563\n",
            "Epoch 295/500, Average Loss: 1.0559\n",
            "Epoch 296/500, Average Loss: 1.0538\n",
            "Epoch 297/500, Average Loss: 1.0553\n",
            "Epoch 298/500, Average Loss: 1.0881\n",
            "Epoch 299/500, Average Loss: 1.0675\n",
            "Epoch 300/500, Average Loss: 1.0716\n",
            "Epoch 301/500, Average Loss: 1.0605\n",
            "Epoch 302/500, Average Loss: 1.0564\n",
            "Epoch 303/500, Average Loss: 1.0550\n",
            "Epoch 304/500, Average Loss: 1.0529\n",
            "Epoch 305/500, Average Loss: 1.0527\n",
            "Epoch 306/500, Average Loss: 1.0506\n",
            "Epoch 307/500, Average Loss: 1.0499\n",
            "Epoch 308/500, Average Loss: 1.0491\n",
            "Epoch 309/500, Average Loss: 1.0504\n",
            "Epoch 310/500, Average Loss: 1.0498\n",
            "Epoch 311/500, Average Loss: 1.0487\n",
            "Epoch 312/500, Average Loss: 1.0482\n",
            "Epoch 313/500, Average Loss: 1.0471\n",
            "Epoch 314/500, Average Loss: 1.0476\n",
            "Epoch 315/500, Average Loss: 1.0476\n",
            "Epoch 316/500, Average Loss: 1.0464\n",
            "Epoch 317/500, Average Loss: 1.0465\n",
            "Epoch 318/500, Average Loss: 1.0464\n",
            "Epoch 319/500, Average Loss: 1.0464\n",
            "Epoch 320/500, Average Loss: 1.0462\n",
            "Epoch 321/500, Average Loss: 1.0459\n",
            "Epoch 322/500, Average Loss: 1.0445\n",
            "Epoch 323/500, Average Loss: 1.0457\n",
            "Epoch 324/500, Average Loss: 1.0450\n",
            "Epoch 325/500, Average Loss: 1.0449\n",
            "Epoch 326/500, Average Loss: 1.0435\n",
            "Epoch 327/500, Average Loss: 1.0452\n",
            "Epoch 328/500, Average Loss: 1.0438\n",
            "Epoch 329/500, Average Loss: 1.0436\n",
            "Epoch 330/500, Average Loss: 1.0433\n",
            "Epoch 331/500, Average Loss: 1.0433\n",
            "Epoch 332/500, Average Loss: 1.0441\n",
            "Epoch 333/500, Average Loss: 1.0434\n",
            "Epoch 334/500, Average Loss: 1.0426\n",
            "Epoch 335/500, Average Loss: 1.0427\n",
            "Epoch 336/500, Average Loss: 1.0430\n",
            "Epoch 337/500, Average Loss: 1.0424\n",
            "Epoch 338/500, Average Loss: 1.0416\n",
            "Epoch 339/500, Average Loss: 1.0403\n",
            "Epoch 340/500, Average Loss: 1.0404\n",
            "Epoch 341/500, Average Loss: 1.0412\n",
            "Epoch 342/500, Average Loss: 1.0428\n",
            "Epoch 343/500, Average Loss: 1.0425\n",
            "Epoch 344/500, Average Loss: 1.0414\n",
            "Epoch 345/500, Average Loss: 1.0409\n",
            "Epoch 346/500, Average Loss: 1.0405\n",
            "Epoch 347/500, Average Loss: 1.0391\n",
            "Epoch 348/500, Average Loss: 1.0391\n",
            "Epoch 349/500, Average Loss: 1.0404\n",
            "Epoch 350/500, Average Loss: 1.0423\n",
            "Epoch 351/500, Average Loss: 1.0408\n",
            "Epoch 352/500, Average Loss: 1.0402\n",
            "Epoch 353/500, Average Loss: 1.0391\n",
            "Epoch 354/500, Average Loss: 1.0416\n",
            "Epoch 355/500, Average Loss: 1.0395\n",
            "Epoch 356/500, Average Loss: 1.0395\n",
            "Epoch 357/500, Average Loss: 1.0382\n",
            "Epoch 358/500, Average Loss: 1.0393\n",
            "Epoch 359/500, Average Loss: 1.0401\n",
            "Epoch 360/500, Average Loss: 1.0401\n",
            "Epoch 361/500, Average Loss: 1.0399\n",
            "Epoch 362/500, Average Loss: 1.0403\n",
            "Epoch 363/500, Average Loss: 1.0410\n",
            "Epoch 364/500, Average Loss: 1.0405\n",
            "Epoch 365/500, Average Loss: 1.0399\n",
            "Epoch 366/500, Average Loss: 1.0390\n",
            "Epoch 367/500, Average Loss: 1.0398\n",
            "Epoch 368/500, Average Loss: 1.0397\n",
            "Epoch 369/500, Average Loss: 1.0381\n",
            "Epoch 370/500, Average Loss: 1.0371\n",
            "Epoch 371/500, Average Loss: 1.0379\n",
            "Epoch 372/500, Average Loss: 1.0392\n",
            "Epoch 373/500, Average Loss: 1.0389\n",
            "Epoch 374/500, Average Loss: 1.0380\n",
            "Epoch 375/500, Average Loss: 1.0391\n",
            "Epoch 376/500, Average Loss: 1.0384\n",
            "Epoch 377/500, Average Loss: 1.0382\n",
            "Epoch 378/500, Average Loss: 1.0368\n",
            "Epoch 379/500, Average Loss: 1.0374\n",
            "Epoch 380/500, Average Loss: 1.0386\n",
            "Epoch 381/500, Average Loss: 1.0379\n",
            "Epoch 382/500, Average Loss: 1.0385\n",
            "Epoch 383/500, Average Loss: 1.0387\n",
            "Epoch 384/500, Average Loss: 1.0376\n",
            "Epoch 385/500, Average Loss: 1.0377\n",
            "Epoch 386/500, Average Loss: 1.0370\n",
            "Epoch 387/500, Average Loss: 1.0364\n",
            "Epoch 388/500, Average Loss: 1.0367\n",
            "Epoch 389/500, Average Loss: 1.0366\n",
            "Epoch 390/500, Average Loss: 1.0365\n",
            "Epoch 391/500, Average Loss: 1.0366\n",
            "Epoch 392/500, Average Loss: 1.0367\n",
            "Epoch 393/500, Average Loss: 1.0362\n",
            "Epoch 394/500, Average Loss: 1.0370\n",
            "Epoch 395/500, Average Loss: 1.0354\n",
            "Epoch 396/500, Average Loss: 1.0356\n",
            "Epoch 397/500, Average Loss: 1.0338\n",
            "Epoch 398/500, Average Loss: 1.0352\n",
            "Epoch 399/500, Average Loss: 1.0353\n",
            "Epoch 400/500, Average Loss: 1.0360\n",
            "Epoch 401/500, Average Loss: 1.0380\n",
            "Epoch 402/500, Average Loss: 1.0359\n",
            "Epoch 403/500, Average Loss: 1.0375\n",
            "Epoch 404/500, Average Loss: 1.0365\n",
            "Epoch 405/500, Average Loss: 1.0372\n",
            "Epoch 406/500, Average Loss: 1.0365\n",
            "Epoch 407/500, Average Loss: 1.0368\n",
            "Epoch 408/500, Average Loss: 1.0366\n",
            "Epoch 409/500, Average Loss: 1.0355\n",
            "Epoch 410/500, Average Loss: 1.0363\n",
            "Epoch 411/500, Average Loss: 1.0385\n",
            "Epoch 412/500, Average Loss: 1.0376\n",
            "Epoch 413/500, Average Loss: 1.0374\n",
            "Epoch 414/500, Average Loss: 1.0379\n",
            "Epoch 415/500, Average Loss: 1.0369\n",
            "Epoch 416/500, Average Loss: 1.0357\n",
            "Epoch 417/500, Average Loss: 1.0346\n",
            "Epoch 418/500, Average Loss: 1.0356\n",
            "Epoch 419/500, Average Loss: 1.0354\n",
            "Epoch 420/500, Average Loss: 1.0360\n",
            "Epoch 421/500, Average Loss: 1.0341\n",
            "Epoch 422/500, Average Loss: 1.0335\n",
            "Epoch 423/500, Average Loss: 1.0336\n",
            "Epoch 424/500, Average Loss: 1.0355\n",
            "Epoch 425/500, Average Loss: 1.0340\n",
            "Epoch 426/500, Average Loss: 1.0338\n",
            "Epoch 427/500, Average Loss: 1.0331\n",
            "Epoch 428/500, Average Loss: 1.0324\n",
            "Epoch 429/500, Average Loss: 1.0334\n",
            "Epoch 430/500, Average Loss: 1.0348\n",
            "Epoch 431/500, Average Loss: 1.0347\n",
            "Epoch 432/500, Average Loss: 1.0331\n",
            "Epoch 433/500, Average Loss: 1.0353\n",
            "Epoch 434/500, Average Loss: 1.0336\n",
            "Epoch 435/500, Average Loss: 1.0341\n",
            "Epoch 436/500, Average Loss: 1.0332\n",
            "Epoch 437/500, Average Loss: 1.0346\n",
            "Epoch 438/500, Average Loss: 1.0337\n",
            "Epoch 439/500, Average Loss: 1.0349\n",
            "Epoch 440/500, Average Loss: 1.0337\n",
            "Epoch 441/500, Average Loss: 1.0342\n",
            "Epoch 442/500, Average Loss: 1.0347\n",
            "Epoch 443/500, Average Loss: 1.0339\n",
            "Epoch 444/500, Average Loss: 1.0323\n",
            "Epoch 445/500, Average Loss: 1.0335\n",
            "Epoch 446/500, Average Loss: 1.0332\n",
            "Epoch 447/500, Average Loss: 1.0338\n",
            "Epoch 448/500, Average Loss: 1.0330\n",
            "Epoch 449/500, Average Loss: 1.0331\n",
            "Epoch 450/500, Average Loss: 1.0317\n",
            "Epoch 451/500, Average Loss: 1.0323\n",
            "Epoch 452/500, Average Loss: 1.0335\n",
            "Epoch 453/500, Average Loss: 1.0336\n",
            "Epoch 454/500, Average Loss: 1.0311\n",
            "Epoch 455/500, Average Loss: 1.0318\n",
            "Epoch 456/500, Average Loss: 1.0314\n",
            "Epoch 457/500, Average Loss: 1.0319\n",
            "Epoch 458/500, Average Loss: 1.0402\n",
            "Epoch 459/500, Average Loss: 1.0471\n",
            "Epoch 460/500, Average Loss: 1.0433\n",
            "Epoch 461/500, Average Loss: 1.0404\n",
            "Epoch 462/500, Average Loss: 1.0396\n",
            "Epoch 463/500, Average Loss: 1.0358\n",
            "Epoch 464/500, Average Loss: 1.0342\n",
            "Epoch 465/500, Average Loss: 1.0337\n",
            "Epoch 466/500, Average Loss: 1.0344\n",
            "Epoch 467/500, Average Loss: 1.0341\n",
            "Epoch 468/500, Average Loss: 1.0327\n",
            "Epoch 469/500, Average Loss: 1.0316\n",
            "Epoch 470/500, Average Loss: 1.0319\n",
            "Epoch 471/500, Average Loss: 1.0325\n",
            "Epoch 472/500, Average Loss: 1.0312\n",
            "Epoch 473/500, Average Loss: 1.0303\n",
            "Epoch 474/500, Average Loss: 1.0296\n",
            "Epoch 475/500, Average Loss: 1.0312\n",
            "Epoch 476/500, Average Loss: 1.0309\n",
            "Epoch 477/500, Average Loss: 1.0299\n",
            "Epoch 478/500, Average Loss: 1.0306\n",
            "Epoch 479/500, Average Loss: 1.0304\n",
            "Epoch 480/500, Average Loss: 1.0312\n",
            "Epoch 481/500, Average Loss: 1.0313\n",
            "Epoch 482/500, Average Loss: 1.0324\n",
            "Epoch 483/500, Average Loss: 1.0313\n",
            "Epoch 484/500, Average Loss: 1.0298\n",
            "Epoch 485/500, Average Loss: 1.0293\n",
            "Epoch 486/500, Average Loss: 1.0306\n",
            "Epoch 487/500, Average Loss: 1.0300\n",
            "Epoch 488/500, Average Loss: 1.0306\n",
            "Epoch 489/500, Average Loss: 1.0324\n",
            "Epoch 490/500, Average Loss: 1.0314\n",
            "Epoch 491/500, Average Loss: 1.0295\n",
            "Epoch 492/500, Average Loss: 1.0306\n",
            "Epoch 493/500, Average Loss: 1.0300\n",
            "Epoch 494/500, Average Loss: 1.0298\n",
            "Epoch 495/500, Average Loss: 1.0305\n",
            "Epoch 496/500, Average Loss: 1.0313\n",
            "Epoch 497/500, Average Loss: 1.0301\n",
            "Epoch 498/500, Average Loss: 1.0287\n",
            "Epoch 499/500, Average Loss: 1.0293\n",
            "Epoch 500/500, Average Loss: 1.0284\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtMyBO3HujSNV2SklxC9MD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}